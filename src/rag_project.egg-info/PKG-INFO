Metadata-Version: 2.4
Name: rag_project
Version: 0.1.0
Summary: A local RAG pipeline for Q&A with your documents.
Author-email: Gemini <gemini@google.com>
Project-URL: Homepage, https://github.com/google/gemini
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# Local RAG Pipeline with Phi-3 and ChromaDB
## ðŸ“– Overview
This project implements a complete, local-first Retrieval-Augmented Generation (RAG) pipeline. It's designed to answer questions based on a collection of PDF documents you provide. The entire processâ€”from data ingestion and embedding to querying and generationâ€”runs on your local machine, ensuring data privacy and requiring no cloud services or paid API keys.

The core of this project is to demonstrate a modern data engineering workflow applied to the domain of Large Language Models (LLMs). It showcases the processing of unstructured data (PDFs), structuring it for efficient retrieval using a vector database (ChromaDB), and interfacing with a local LLM (Microsoft's Phi-3) to generate context-aware answers.

This project directly demonstrates skills relevant to Data Engineering, including:

- ETL/ELT Processes: A clear ETL (Extract, Transform, Load) process is implemented:
    - Extract: Text is extracted from PDF documents.
    - Transform: The text is chunked and converted into vector embeddings.
    - Load: The chunks and their embeddings are loaded into a persistent ChromaDB vector database.

- Python, SQL-like Queries & Automation: The pipeline is built in Python, uses a vector database, and is automated with a Makefile.

- Unstructured Data & RAG: It directly tackles the challenge of processing unstructured text and building a RAG pipeline, a highly relevant skill in the modern data and AI landscape.

## ðŸ“œ Table of Contents
1. âœ¨ Features 

2. ðŸ› ï¸ Technologies Used

3. ðŸ“‚ Project Structure

4. ðŸš€ Setup and Installation

5. â–¶ï¸ How to Run the Project

6. ðŸ’¡ Troubleshooting Common Issues

## 1. âœ¨ Features

- 100% Local: All components run locally. No API keys or internet connection required after the initial setup.

- PDF Document Source: Easily add your own PDF documents to the docs folder to build a custom knowledge base.

- Persistent Vector Storage: Uses ChromaDB to save the document index locally, so you only need to process your documents once.

- High-Quality Local LLM: Leverages the powerful and efficient phi3:mini model run via Ollama.

- Interactive Q&A: Provides a simple command-line interface to ask questions about your documents.

## 2. ðŸ› ï¸ Technologies Used

| Category | Technology | Purpose | 
| ----------- | ----------- | ----------- |
| Language | Python 3 | Core programming language for the pipeline |
| LLM Engine | Ollama | For serving the local Large Language Model |
| Language Model | phi3:mini | The efficient LLM used for answer generation |
| Vector Database | ChromaDB | Storing and querying text chunks and embeddings |
| Embedding Model | all-MiniLM-L6-v2 | Generating semantic vector embeddings from text |
| Core Libraries | ollama, chromadb, pypdf | Interacting with the tech stack components |
| Automation | Makefile | Streamlining setup, indexing, and querying |

## 3. ðŸ“‚ Project Structure

```
local_rag_project/
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ example.pdf         # Place source PDF documents here
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rag_pipeline.py     # The main script
â”‚
â”œâ”€â”€ chroma_db/              # Directory for the persistent vector database
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                # Automation for setup and execution
â”œâ”€â”€ README.md               # Documentation
â””â”€â”€ requirements.txt        # Project dependencies
```

## 4. ðŸš€ Setup (local) and Installation

### Prerequisite: Install Ollama
This project requires Ollama to run the local LLM.

- Download and install Ollama from the official website.

- After installation, launch the Ollama application. You should see a llama icon in your system's menu bar.

- Pull the ```phi3:mini``` model by running the following command in your terminal:
```
ollama pull phi3:mini
```
> **Note: The Ollama application must be running in the background for the project to work.**

### Project Setup
These commands will set up a virtual environment and install the necessary Python packages.

- Clone the repository (if you haven't already):
```
git clone https://github.com/sephcodes66/Smart_Dita_Factory 
cd local_rag_project
```
- Run the setup command from the Makefile:
```
make setup
```
This will create a ```venv``` directory and install all packages listed in ```requirements.txt```.

## 5. â–¶ï¸ How to Run the Project
The ```Makefile``` simplifies the entire workflow into two main steps.

- Step 1: Index Your Documents
This command processes all PDFs in the ./docs folder and saves them into the local vector database. You only need to run this once, or whenever you add, remove, or change your source documents.
```
make index
```
- Step 2: Ask Questions
This command starts the interactive query session, allowing you to ask questions about your indexed documents.
```
make query
```
The script will then prompt you to enter your question. To exit the session, simply type exit and press Enter.

## 6. ðŸ’¡ Troubleshooting Common Issues
- Error: Connection refused
    - Cause: The Ollama application is not running in the background.
    - Solution: Launch the Ollama application from your system's Application folder. Verify it's running by checking for the llama icon in your menu bar or by running ollama list in a new terminal.

- Old or incorrect sources appear in the results.
    - Cause: The ChromaDB database (./chroma_db) is persistent and contains data from previously indexed files that you may have since deleted.
    - Solution: To create a fresh index, first delete the old database directory and then re-run the indexing command.

> **Important: This will permanently delete your existing index.**
[(Back to top)](##table-of-contents)
